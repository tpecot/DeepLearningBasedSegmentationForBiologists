{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n",
    "\n",
    "The first step is to load the functions from utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import extract_channels, divide_images, merge_scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract channel(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# folder with input images\n",
    "input_image_dir = '../../Data/Polyps_multiplexedImages/'\n",
    "\n",
    "# folder with output images\n",
    "output_image_dir = './datasets/Nuclei/test/'\n",
    "\n",
    "# channels to be extracted\n",
    "channels = [0]\n",
    "\n",
    "# call function\n",
    "extract_channels(input_image_dir, output_image_dir, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide image into sub-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# folder with input images\n",
    "input_image_dir = '../../Data/AnnotatedNuclei/images/'\n",
    "\n",
    "# folder with output images\n",
    "output_image_dir = './datasets/Nuclei/training/images/'\n",
    "\n",
    "# channels to be extracted\n",
    "width_divider = 4\n",
    "height_divider = 3\n",
    "\n",
    "# call function\n",
    "divide_images(input_image_dir, output_image_dir, width_divider, height_divider)"
   ]
  },]
 "nbformat": 4,
 "nbformat_minor": 1
}
